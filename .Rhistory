f_var <- prefixes[which.max(nchar(prefixes))]
f_lvl <- sub(f_var, "", nc)
training_set <- drop_level_to_na(training_set, f_var, f_lvl)
validation_set <- drop_level_to_na(validation_set, f_var, f_lvl)
}
}
}
# Check VIF values
vif_values <- vif(initial_model)
print(vif_values)
# Step 1: Set any 'RRNn' values in training to NA (just in case)
training_set$Condition2 <- as.character(training_set$Condition2)
training_set$Condition2[training_set$Condition2 == "RRNn"] <- NA
training_set$Condition2 <- factor(training_set$Condition2)
# Step 2: Drop 'RRNn' from levels by recreating factor (droplevels is also fine)
training_set$Condition2 <- droplevels(training_set$Condition2)
# Step 3: Repeat your validation cleaning step
valid_lvls <- levels(training_set$Condition2)
col_char <- as.character(validation_set$Condition2)
col_char[!col_char %in% valid_lvls] <- NA
validation_set$Condition2 <- factor(col_char, levels = valid_lvls)
validation_set$Condition2 <- droplevels(validation_set$Condition2)
# Step 4: Confirm 'RRNn' is GONE from both values and levels!
print(unique(as.character(training_set$Condition2)))
print(levels(training_set$Condition2))
print(unique(as.character(validation_set$Condition2)))
print(levels(validation_set$Condition2))
table(validation_set$Condition2, useNA = "always")
# Predict on validation set and compute RMSE
val_preds <- predict(initial_model, newdata = validation_set)
clean_factor_levels <- function(train, valid) {
for (col in intersect(names(train), names(valid))) {
if (is.factor(train[[col]])) {
valid_lvls <- levels(train[[col]])
# Set invalid levels to NA
valid_char <- as.character(valid[[col]])
valid_char[!valid_char %in% valid_lvls] <- NA
valid[[col]] <- factor(valid_char, levels = valid_lvls)
valid[[col]] <- droplevels(valid[[col]])
}
}
valid
}
validation_set <- clean_factor_levels(training_set, validation_set)
test <- clean_factor_levels(training_set, test)
initial_model <- lm(SalePrice ~ ., data = training_set)
val_preds <- predict(initial_model, newdata = validation_set)
print(unique(as.character(training_set$RoofMatl)))
print(levels(training_set$RoofMatl))
print(unique(as.character(validation_set$RoofMatl)))
print(levels(validation_set$RoofMatl))
table(validation_set$RoofMatl, useNA = "always")
training_set$RoofMatl <- droplevels(training_set$RoofMatl)
allowed_lvls <- levels(training_set$RoofMatl)
val_roof <- as.character(validation_set$RoofMatl)
val_roof[!val_roof %in% allowed_lvls] <- NA
validation_set$RoofMatl <- factor(val_roof, levels = allowed_lvls)
validation_set$RoofMatl <- droplevels(validation_set$RoofMatl)
print(unique(as.character(training_set$RoofMatl)))
print(levels(training_set$RoofMatl))
print(unique(as.character(validation_set$RoofMatl)))
print(levels(validation_set$RoofMatl))
table(validation_set$RoofMatl, useNA = "always")
initial_model <- lm(SalePrice ~ ., data = training_set)
val_preds <- predict(initial_model, newdata = validation_set)
# For every factor, set "unknown" levels in validation to NA and drop unused levels
fix_validation_factors <- function(train, valid) {
for (col in intersect(names(train), names(valid))) {
if (is.factor(train[[col]])) {
allowed_lvls <- levels(droplevels(train[[col]]))
# Set any values in validation not present in training to NA
val_char <- as.character(valid[[col]])
val_char[!val_char %in% allowed_lvls] <- NA
valid[[col]] <- factor(val_char, levels = allowed_lvls)
valid[[col]] <- droplevels(valid[[col]])
}
}
valid
}
# Now run this:
validation_set <- fix_validation_factors(training_set, validation_set)
test <- fix_validation_factors(training_set, test)
val_preds <- predict(initial_model, newdata = validation_set)
training_set <- fix_validation_factors(training_set, training_set)
validation_set <- fix_validation_factors(training_set, validation_set)
test <- fix_validation_factors(training_set, test)
initial_model <- lm(SalePrice ~ ., data = training_set)
val_preds <- predict(initial_model, newdata = validation_set)
for (col in names(training_set)) {
if (is.factor(training_set[[col]])) {
training_set[[col]] <- droplevels(training_set[[col]])
}
}
for (col in intersect(names(training_set), names(validation_set))) {
if (is.factor(training_set[[col]])) {
allowed_lvls <- levels(training_set[[col]])
val_char <- as.character(validation_set[[col]])
val_char[!val_char %in% allowed_lvls] <- NA
validation_set[[col]] <- factor(val_char, levels = allowed_lvls)
validation_set[[col]] <- droplevels(validation_set[[col]])
}
}
initial_model <- lm(SalePrice ~ ., data = training_set)
val_preds <- predict(initial_model, newdata = validation_set)
cat("Unique values in training_set$Foundation:\n")
print(unique(as.character(training_set$Foundation)))
cat("Levels in training_set$Foundation:\n")
print(levels(training_set$Foundation))
cat("Unique values in validation_set$Foundation:\n")
print(unique(as.character(validation_set$Foundation)))
cat("Levels in validation_set$Foundation:\n")
print(levels(validation_set$Foundation))
cat("Table of validation_set$Foundation:\n")
print(table(validation_set$Foundation, useNA = "always"))
table(training_set$Foundation)
initial_model <- lm(SalePrice ~ ., data = training_set)
str(initial_model$xlevels)
# Load required libraries
library(tidyverse)
library(ggplot2)
library(dplyr)
library(randomForest)
library(xgboost)
library(caret)
library(car)
# Set seed for reproducibility
set.seed(123)
# Set working directory
setwd("~/Documents/DATA522/House-Prices-Regression-Final-Project")
# Load the datasets from data/raw folder
train <- read_csv("data/raw/train.csv")
test <- read_csv("data/raw/test.csv")
# Initial EDA
ggplot(train, aes(x = SalePrice)) +
geom_histogram(fill = "skyblue", bins = 30) +
theme_minimal() +
labs(title = "Distribution of Sale Price")
# Log transform SalePrice (do this before any other transformations)
train$SalePrice <- log(train$SalePrice)
# Plot log-transformed SalePrice
ggplot(train, aes(x = SalePrice)) +
geom_histogram(fill = "seagreen", bins = 30) +
theme_minimal() +
labs(title = "Log-Transformed Distribution of SalePrice")
# Data Preparation Function
prepare_data <- function(df, is_training = TRUE) {
# Handle missing values
df <- df %>%
mutate(across(where(is.numeric),
~ifelse(is.na(.), median(., na.rm = TRUE), .))) %>%
mutate(across(where(is.character),
~ifelse(is.na(.), "None", .)))
# Feature engineering
df <- df %>%
mutate(
HouseAge = YrSold - YearBuilt,
RemodAge = YrSold - YearRemodAdd,
TotalSF = TotalBsmtSF + `1stFlrSF` + `2ndFlrSF`,
Bathrooms = FullBath + 0.5 * HalfBath,
TotalPorchSF = WoodDeckSF + OpenPorchSF + EnclosedPorch + `3SsnPorch` + ScreenPorch
)
# Convert character columns to factors
df <- df %>%
mutate(across(where(is.character), factor))
return(df)
}
# Prepare training and test data
train <- prepare_data(train, is_training = TRUE)
test <- prepare_data(test, is_training = FALSE)
# Create training/validation split
split <- createDataPartition(train$SalePrice, p = 0.8, list = FALSE)
training_set <- train[split, ]
validation_set <- train[-split, ]
# --- NEW FUNCTION: Clean and align ALL factor columns for validation and test ---
clean_and_align_factors <- function(train, valid) {
for (col in intersect(names(train), names(valid))) {
if (is.factor(train[[col]])) {
train[[col]] <- droplevels(train[[col]])
allowed_lvls <- levels(train[[col]])
val_char <- as.character(valid[[col]])
val_char[!val_char %in% allowed_lvls] <- NA
valid[[col]] <- factor(val_char, levels = allowed_lvls)
valid[[col]] <- droplevels(valid[[col]])
}
}
for (col in names(train)) {
if (is.factor(train[[col]])) {
train[[col]] <- droplevels(train[[col]])
}
}
return(list(train = train, valid = valid))
}
# -------------------------------------------------------------------------------
# --- NEW: Clean and align all factors ---
cleaned <- clean_and_align_factors(training_set, validation_set)
training_set <- cleaned$train
validation_set <- cleaned$valid
cleaned_test <- clean_and_align_factors(training_set, test)
training_set <- cleaned_test$train   # usually unchanged, but for safety
test <- cleaned_test$valid
# Check correlations
numeric_vars <- training_set %>%
select_if(is.numeric) %>%
names()
correlation_matrix <- cor(training_set[numeric_vars], use = "complete.obs")
high_correlations <- which(abs(correlation_matrix) > 0.8 &
correlation_matrix != 1 &
!grepl("SalePrice", rownames(correlation_matrix)),
arr.ind = TRUE)
# Remove highly correlated numeric variables (> 0.8), don't drop SalePrice
numeric_vars <- training_set %>% select(where(is.numeric)) %>% names()
correlation_matrix <- cor(training_set[numeric_vars], use = "complete.obs")
high_correlations <- which(abs(correlation_matrix) > 0.8 &
correlation_matrix != 1, arr.ind = TRUE)
if(length(high_correlations) > 0) {
vars_to_remove <- unique(rownames(correlation_matrix)[high_correlations[,1]])
vars_to_remove <- setdiff(vars_to_remove, "SalePrice") # Don't drop target
vars_to_remove <- vars_to_remove[vars_to_remove %in% names(training_set)]
training_set <- training_set %>% select(-all_of(vars_to_remove))
validation_set <- validation_set %>% select(-all_of(vars_to_remove))
}
# Remove zero-variance predictors
nzv <- nearZeroVar(training_set, saveMetrics = TRUE)
if (any(nzv$zeroVar)) {
training_set <- training_set[, !nzv$zeroVar]
validation_set <- validation_set[, names(training_set)]
test <- test[, names(training_set)]
}
# Remove Aliased Variables (perfect collinearity)
initial_model <- lm(SalePrice ~ ., data = training_set)
alias_model <- alias(initial_model)
if(!is.null(alias_model$Complete)) {
aliased_vars <- rownames(alias_model$Complete)
aliased_vars <- aliased_vars[aliased_vars %in% names(training_set)]
if(length(aliased_vars) > 0) {
training_set <- training_set %>% select(-all_of(aliased_vars))
validation_set <- validation_set %>% select(-all_of(aliased_vars))
test <- test %>% select(-all_of(aliased_vars))
initial_model <- lm(SalePrice ~ ., data = training_set)
}
}
repeat {
aliased_vars <- rownames(alias(initial_model)$Complete)
aliased_vars <- aliased_vars[aliased_vars %in% names(training_set)]
if(length(aliased_vars) == 0) break
training_set <- training_set %>% select(-all_of(aliased_vars))
validation_set <- validation_set %>% select(-all_of(aliased_vars))
test <- test %>% select(-all_of(aliased_vars))
initial_model <- lm(SalePrice ~ ., data = training_set)
}
# --- REMOVE factors with only one level (if any remain) ---
one_level_factors <- names(training_set)[sapply(training_set, function(x) is.factor(x) && length(unique(na.omit(x))) < 2)]
if(length(one_level_factors) > 0) {
training_set <- training_set[, !names(training_set) %in% one_level_factors]
validation_set <- validation_set[, !names(validation_set) %in% one_level_factors]
test <- test[, !names(test) %in% one_level_factors]
}
# Remove MiscVal and MiscFeature if present (based on earlier process)
to_drop <- c("MiscVal", "MiscFeature")
to_drop <- to_drop[to_drop %in% names(training_set)]
if(length(to_drop) > 0) {
training_set <- training_set %>% select(-all_of(to_drop))
validation_set <- validation_set %>% select(-all_of(to_drop))
test <- test %>% select(-all_of(to_drop))
}
# Final initial model (for VIF and baseline)
initial_model <- lm(SalePrice ~ ., data = training_set)
# Print the names of aliased coefficients seen by the model
na_coefs <- names(which(is.na(coef(initial_model))))
print(na_coefs)
# Check VIF values
vif_values <- vif(initial_model)
# Load required libraries
library(tidyverse)
library(ggplot2)
library(dplyr)
library(randomForest)
library(xgboost)
library(caret)
library(car)
set.seed(123)
setwd("~/Documents/DATA522/House-Prices-Regression-Final-Project")
train <- read_csv("data/raw/train.csv")
test <- read_csv("data/raw/test.csv")
# EDA (optional)
# ...
# Log transform SalePrice (do this before any other transformations)
train$SalePrice <- log(train$SalePrice)
prepare_data <- function(df) {
df <- df %>%
mutate(across(where(is.numeric), ~ifelse(is.na(.), median(., na.rm = TRUE), .))) %>%
mutate(across(where(is.character), ~ifelse(is.na(.), "None", .))) %>%
mutate(
HouseAge = YrSold - YearBuilt,
RemodAge = YrSold - YearRemodAdd,
TotalSF = TotalBsmtSF + `1stFlrSF` + `2ndFlrSF`,
Bathrooms = FullBath + 0.5 * HalfBath,
TotalPorchSF = WoodDeckSF + OpenPorchSF + EnclosedPorch + `3SsnPorch` + ScreenPorch
) %>%
mutate(across(where(is.character), factor))
return(df)
}
train <- prepare_data(train)
test <- prepare_data(test)
split <- createDataPartition(train$SalePrice, p = 0.8, list = FALSE)
training_set <- train[split, ]
validation_set <- train[-split, ]
# Remove highly correlated numeric variables
numeric_vars <- training_set %>% select(where(is.numeric)) %>% names()
correlation_matrix <- cor(training_set[numeric_vars], use = "complete.obs")
high_correlations <- which(abs(correlation_matrix) > 0.8 & correlation_matrix != 1, arr.ind = TRUE)
if(length(high_correlations) > 0) {
vars_to_remove <- unique(rownames(correlation_matrix)[high_correlations[,1]])
vars_to_remove <- setdiff(vars_to_remove, "SalePrice")
vars_to_remove <- vars_to_remove[vars_to_remove %in% names(training_set)]
training_set <- training_set %>% select(-all_of(vars_to_remove))
validation_set <- validation_set %>% select(-all_of(vars_to_remove))
test <- test %>% select(-all_of(vars_to_remove))
}
# Remove zero-variance predictors
nzv <- nearZeroVar(training_set, saveMetrics = TRUE)
if (any(nzv$zeroVar)) {
training_set <- training_set[, !nzv$zeroVar]
validation_set <- validation_set[, names(training_set)]
test <- test[, names(training_set)]
}
# Remove MiscVal and MiscFeature if present
to_drop <- c("MiscVal", "MiscFeature")
to_drop <- to_drop[to_drop %in% names(training_set)]
if(length(to_drop) > 0) {
training_set <- training_set %>% select(-all_of(to_drop))
validation_set <- validation_set %>% select(-all_of(to_drop))
test <- test %>% select(-all_of(to_drop))
}
# Remove factors with only one level (if any remain)
one_level_factors <- names(training_set)[sapply(training_set, function(x) is.factor(x) && length(unique(na.omit(x))) < 2)]
if(length(one_level_factors) > 0) {
training_set <- training_set[, !names(training_set) %in% one_level_factors]
validation_set <- validation_set[, !names(validation_set) %in% one_level_factors]
test <- test[, !names(test) %in% one_level_factors]
}
# Remove aliased variables (perfect collinearity)
library(car)
repeat {
initial_model <- lm(SalePrice ~ ., data = training_set)
aliased_vars <- rownames(alias(initial_model)$Complete)
aliased_vars <- aliased_vars[aliased_vars %in% names(training_set)]
if(length(aliased_vars) == 0) break
training_set <- training_set %>% select(-all_of(aliased_vars))
validation_set <- validation_set %>% select(-all_of(aliased_vars))
test <- test %>% select(-all_of(aliased_vars))
}
# --- THE ONLY ALIGNMENT FUNCTION AND CALLS YOU NEED ---
clean_and_align_factors <- function(train, valid) {
for (col in intersect(names(train), names(valid))) {
if (is.factor(train[[col]])) {
train[[col]] <- droplevels(train[[col]])
allowed_lvls <- levels(train[[col]])
val_char <- as.character(valid[[col]])
val_char[!val_char %in% allowed_lvls] <- NA
valid[[col]] <- factor(val_char, levels = allowed_lvls)
valid[[col]] <- droplevels(valid[[col]])
}
}
for (col in names(train)) {
if (is.factor(train[[col]])) {
train[[col]] <- droplevels(train[[col]])
}
}
return(list(train = train, valid = valid))
}
cleaned <- clean_and_align_factors(training_set, validation_set)
training_set <- cleaned$train
validation_set <- cleaned$valid
cleaned_test <- clean_and_align_factors(training_set, test)
training_set <- cleaned_test$train # usually unchanged, but for safety
test <- cleaned_test$valid
# --- NOW FIT MODEL AND PREDICT ---
initial_model <- lm(SalePrice ~ ., data = training_set)
val_preds <- predict(initial_model, newdata = validation_set)
val_preds_exp <- exp(val_preds)
actual_exp <- exp(validation_set$SalePrice)
rmse <- sqrt(mean((val_preds_exp - actual_exp)^2))
cat("Validation RMSE:", rmse, "\n")
# Prepare test data for prediction
test_pred_input <- test %>% select(names(training_set))
# Load required libraries
library(tidyverse)
library(ggplot2)
library(dplyr)
library(randomForest)
library(xgboost)
library(caret)
library(car)
# Set seed for reproducibility
set.seed(123)
# Set working directory
setwd("~/Documents/DATA522/House-Prices-Regression-Final-Project")
# Load the datasets from data/raw folder
train <- read_csv("data/raw/train.csv")
test <- read_csv("data/raw/test.csv")
# Initial EDA: Distribution of SalePrice before transformation
ggplot(train, aes(x = SalePrice)) +
geom_histogram(fill = "skyblue", bins = 30) +
theme_minimal() +
labs(title = "Distribution of Sale Price")
# Log transform SalePrice (do this before any other transformations)
train$SalePrice <- log(train$SalePrice)
# Plot log-transformed SalePrice
ggplot(train, aes(x = SalePrice)) +
geom_histogram(fill = "seagreen", bins = 30) +
theme_minimal() +
labs(title = "Log-Transformed Distribution of SalePrice")
# Data Preparation Function
prepare_data <- function(df) {
# Handle missing values
df <- df %>%
mutate(across(where(is.numeric), ~ifelse(is.na(.), median(., na.rm = TRUE), .))) %>%
mutate(across(where(is.character), ~ifelse(is.na(.), "None", .))) %>%
# Feature engineering
mutate(
HouseAge = YrSold - YearBuilt,
RemodAge = YrSold - YearRemodAdd,
TotalSF = TotalBsmtSF + `1stFlrSF` + `2ndFlrSF`,
Bathrooms = FullBath + 0.5 * HalfBath,
TotalPorchSF = WoodDeckSF + OpenPorchSF + EnclosedPorch + `3SsnPorch` + ScreenPorch
) %>%
mutate(across(where(is.character), factor))
return(df)
}
# Prepare training and test data
train <- prepare_data(train)
test <- prepare_data(test)
# Create training/validation split
split <- createDataPartition(train$SalePrice, p = 0.8, list = FALSE)
training_set <- train[split, ]
validation_set <- train[-split, ]
# Remove highly correlated numeric variables
numeric_vars <- training_set %>% select(where(is.numeric)) %>% names()
correlation_matrix <- cor(training_set[numeric_vars], use = "complete.obs")
high_correlations <- which(abs(correlation_matrix) > 0.8 & correlation_matrix != 1, arr.ind = TRUE)
if(length(high_correlations) > 0) {
vars_to_remove <- unique(rownames(correlation_matrix)[high_correlations[,1]])
vars_to_remove <- setdiff(vars_to_remove, "SalePrice")
vars_to_remove <- vars_to_remove[vars_to_remove %in% names(training_set)]
training_set <- training_set %>% select(-all_of(vars_to_remove))
validation_set <- validation_set %>% select(-all_of(vars_to_remove))
test <- test %>% select(-all_of(vars_to_remove))
}
# Remove zero-variance predictors
nzv <- nearZeroVar(training_set, saveMetrics = TRUE)
if (any(nzv$zeroVar)) {
training_set <- training_set[, !nzv$zeroVar]
validation_set <- validation_set[, names(training_set)]
test <- test[, names(training_set)]
}
# Remove MiscVal and MiscFeature if present
to_drop <- c("MiscVal", "MiscFeature")
to_drop <- to_drop[to_drop %in% names(training_set)]
if(length(to_drop) > 0) {
training_set <- training_set %>% select(-all_of(to_drop))
validation_set <- validation_set %>% select(-all_of(to_drop))
test <- test %>% select(-all_of(to_drop))
}
# Remove factors with only one level (if any remain)
one_level_factors <- names(training_set)[sapply(training_set, function(x) is.factor(x) && length(unique(na.omit(x))) < 2)]
if(length(one_level_factors) > 0) {
training_set <- training_set[, !names(training_set) %in% one_level_factors]
validation_set <- validation_set[, !names(validation_set) %in% one_level_factors]
test <- test[, !names(test) %in% one_level_factors]
}
# Remove aliased variables (perfect collinearity)
repeat {
initial_model <- lm(SalePrice ~ ., data = training_set)
aliased_vars <- rownames(alias(initial_model)$Complete)
aliased_vars <- aliased_vars[aliased_vars %in% names(training_set)]
if(length(aliased_vars) == 0) break
training_set <- training_set %>% select(-all_of(aliased_vars))
validation_set <- validation_set %>% select(-all_of(aliased_vars))
test <- test %>% select(-all_of(aliased_vars))
}
# Align factor levels between training, validation, and test (final step before modeling)
clean_and_align_factors <- function(train, valid) {
for (col in intersect(names(train), names(valid))) {
if (is.factor(train[[col]])) {
train[[col]] <- droplevels(train[[col]])
allowed_lvls <- levels(train[[col]])
val_char <- as.character(valid[[col]])
val_char[!val_char %in% allowed_lvls] <- NA
valid[[col]] <- factor(val_char, levels = allowed_lvls)
valid[[col]] <- droplevels(valid[[col]])
}
}
for (col in names(train)) {
if (is.factor(train[[col]])) {
train[[col]] <- droplevels(train[[col]])
}
}
return(list(train = train, valid = valid))
}
cleaned <- clean_and_align_factors(training_set, validation_set)
training_set <- cleaned$train
validation_set <- cleaned$valid
cleaned_test <- clean_and_align_factors(training_set, test)
training_set <- cleaned_test$train # usually unchanged, but for safety
test <- cleaned_test$valid
# Final model fit
initial_model <- lm(SalePrice ~ ., data = training_set)
# VIF diagnostics (optional)
vif_values <- vif(initial_model)
source("~/Documents/DATA522/House-Prices-Regression-Final-Project/01_data_setup_and_eda.R")
source("~/Documents/DATA522/House-Prices-Regression-Final-Project/01_data_setup_and_eda.R")
